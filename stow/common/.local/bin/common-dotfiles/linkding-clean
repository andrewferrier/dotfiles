#!/usr/bin/env -S uv run --project ${HOME}/.local/bin/common-dotfiles --script
# vim: set ft=python:
import argparse
import functools
import hashlib
import html
import json
import logging
import os
import pathlib
import re
import sys
import urllib.parse
from collections.abc import Callable
from dataclasses import dataclass
from datetime import datetime, timedelta, timezone
from typing import Any, Literal

import requests
from playwright.sync_api import Browser, sync_playwright
from tqdm import tqdm


@dataclass
class ValidationResult:
    is_valid: bool
    reason: str


logger = logging.getLogger("linkding-clean")
logger.propagate = False
logger.setLevel(logging.INFO)  # Changed default level to INFO

syserr_handler = logging.StreamHandler(stream=sys.stderr)
syserr_handler.setLevel(logging.INFO)  # Changed default level to INFO
syserr_handler.setFormatter(logging.Formatter("%(levelname)s: %(message)s"))
logger.addHandler(syserr_handler)


XDG_CACHE_HOME = pathlib.Path(os.getenv("XDG_CACHE_HOME", "~/.cache")).expanduser()
LINKDING_CLEAN_CACHE_DIR = XDG_CACHE_HOME / "linkding-clean"
LINKDING_CLEAN_CACHE_DIR.mkdir(parents=True, exist_ok=True)

HTTP_OK_MIN = 200
HTTP_OK_MAX = 300


def disk_cache(
    expire_time_hours: int = 72,
) -> Callable[..., Callable[..., ValidationResult]]:
    def decorator(
        func: Callable[..., ValidationResult],
    ) -> Callable[..., ValidationResult]:
        @functools.wraps(func)
        def wrapper(self, url: str, *args: Any, **kwargs: Any) -> ValidationResult:  # noqa: ANN001, ANN401
            cache_key = hashlib.sha256(url.encode("utf-8")).hexdigest()
            cache_file = LINKDING_CLEAN_CACHE_DIR / f"{cache_key}.json"

            if cache_file.exists():
                try:
                    with cache_file.open("r") as f:
                        cache_data = json.load(f)

                    cached_timestamp = datetime.fromisoformat(cache_data["timestamp"])

                    if datetime.now(timezone.UTC) - cached_timestamp < timedelta(
                        hours=expire_time_hours
                    ):
                        logger.debug(f"Cache hit for URL: {url} (ID: {cache_key})")
                        return ValidationResult(**cache_data["result"])
                    else:
                        logger.debug(f"Cache expired for URL: {url} (ID: {cache_key}).")
                        cache_file.unlink(missing_ok=True)
                except (json.JSONDecodeError, KeyError, ValueError) as e:
                    logger.warning(
                        f"Error reading cache for {url} (ID: {cache_key}): {e}."
                    )
                    cache_file.unlink(missing_ok=True)

            result = func(self, url, *args, **kwargs)

            with cache_file.open("w") as f:
                json.dump(
                    {
                        "timestamp": datetime.now(timezone.UTC).isoformat(),
                        "result": result.__dict__,  # Serialize dataclass to dict
                        "url": url,
                    },
                    f,
                )

            return result

        return wrapper

    return decorator


class LinkdingAPIClient:
    def __init__(self, base_url: str, token: str) -> None:
        if not base_url.endswith("/"):
            base_url += "/"
        self.base_url = base_url
        self.session = requests.Session()
        self.session.headers.update({"Authorization": f"Token {token}"})
        logger.debug(f"LinkdingAPIClient initialized with base_url: {self.base_url}")

    def get_bookmarks(self, query: str | None = None, *, archived: bool = False) -> Any:  # noqa: ANN401
        endpoint = "archived/" if archived else ""

        params: dict[str, str] = {"limit": "100000"}

        if query is not None:
            params["q"] = query

        url = f"{self.base_url}bookmarks/{endpoint}"
        logger.debug(
            f"Fetching {'archived' if archived else 'unarchived'} bookmarks "
            f"with query '{query}', params '{params}' from: {url}"
        )
        response = self.session.get(url, params=params)
        response.raise_for_status()
        data = response.json()
        return data["results"]

    def get_tags(self) -> Any:  # noqa: ANN401
        url = f"{self.base_url}tags/?limit=100000"
        response = self.session.get(url)
        response.raise_for_status()
        data = response.json()
        return data["results"]

    def archive_bookmark(self, bookmark_id: int) -> None:
        url = f"{self.base_url}bookmarks/{bookmark_id}/archive/"
        response = self.session.post(url)
        response.raise_for_status()
        logger.debug(f"Successfully archived bookmark ID: {bookmark_id}")

    def unarchive_bookmark(self, bookmark_id: int) -> None:
        url = f"{self.base_url}bookmarks/{bookmark_id}/unarchive/"
        response = self.session.post(url)
        response.raise_for_status()
        logger.debug(f"Successfully unarchived bookmark ID: {bookmark_id}")

    def delete_bookmark(self, bookmark_id: int) -> None:
        url = f"{self.base_url}bookmarks/{bookmark_id}/"
        response = self.session.delete(url)
        response.raise_for_status()
        logger.debug(f"Successfully deleted bookmark ID: {bookmark_id}")

    def update_bookmark(
        self,
        bookmark_id: int,
        *,
        title: str | None = None,
        url: str | None = None,
        unread: bool | None = None,
        dry_run: bool = False,
    ) -> None:
        url_path = f"{self.base_url}bookmarks/{bookmark_id}/"

        payload = {}
        if title is not None:
            payload["title"] = title
        if url is not None:
            payload["url"] = url
        if unread is not None:
            payload["unread"] = unread

        if not payload:
            logger.debug(f"No update data provided for bookmark ID: {bookmark_id}")
            return

        if dry_run:
            logger.info(
                f"DRY RUN: Would update bookmark ID: {bookmark_id} with payload: "
                f"{payload}"
            )
        else:
            try:
                logger.info(
                    f"Updating bookmark ID: {bookmark_id} with payload: {payload}"
                )
                response = self.session.patch(url_path, json=payload)
                response.raise_for_status()
                logger.debug(f"Successfully updated bookmark ID: {bookmark_id}")
            except requests.exceptions.RequestException:
                logger.error(  # noqa: TRY400
                    f"Failed to update bookmark ID: {bookmark_id} with payload: "
                    f"{payload}"
                )

    def get_remote_metadata(self, url: str) -> dict[str, Any] | None:
        escaped_url = urllib.parse.quote_plus(url)
        check_url = f"http://192.168.1.10:8084/api/bookmarks/check/?url={escaped_url}"
        logger.debug(f"Checking remote metadata for URL: {url} using {check_url}")
        try:
            response = self.session.get(check_url, timeout=30)
            response.raise_for_status()
            data = response.json()
            return data.get("metadata")
        except requests.exceptions.ReadTimeout:
            logger.warning(
                f"Read timeout when fetching remote metadata for URL: {url}. Skipping."
            )
            return None


def _clean_bookmark_title(
    client: LinkdingAPIClient,
    bookmark: dict,
    dry_run: bool,  # noqa: FBT001
) -> None:
    bookmark_id: int = bookmark["id"]
    original_title: str = bookmark.get("title", "")
    bookmark_url: str = bookmark["url"]

    compare_len = min(len(original_title), len(bookmark_url), 20)

    title_prefix_for_comparison = original_title[:compare_len]
    url_prefix_for_comparison = bookmark_url[:compare_len]

    if (not original_title) or title_prefix_for_comparison == url_prefix_for_comparison:
        try:
            metadata = client.get_remote_metadata(bookmark_url)
            fetched_title = metadata.get("title") if metadata else None

            if fetched_title and fetched_title != original_title:
                if dry_run:
                    logger.info(
                        f"DRY RUN: Would update title for bookmark ID: {bookmark_id} "
                        f"from '{original_title}' to '{fetched_title}' "
                        "(from remote metadata)."
                    )
                else:
                    logger.info(
                        f"Updating title for bookmark ID: {bookmark_id} "
                        f"from '{original_title}' to '{fetched_title}' "
                        "(from remote metadata)."
                    )
                    client.update_bookmark(bookmark_id, title=fetched_title)
        except requests.exceptions.RequestException:
            logger.error(  # noqa: TRY400
                f"Failed to fetch metadata from remote for bookmark ID: {bookmark_id}, "
                f"URL: {bookmark_url}"
            )
        return

    cleaned_title = html.unescape(original_title)

    if cleaned_title != original_title:
        if dry_run:
            logger.info(
                f"DRY RUN: Would update title for bookmark ID: {bookmark_id} "
                f"from '{original_title}' to '{cleaned_title}'",
            )
        else:
            try:
                logger.info(
                    f"Updating title for bookmark ID: {bookmark_id} "
                    f"from '{original_title}' to '{cleaned_title}'",
                )
                client.update_bookmark(bookmark_id, title=cleaned_title)
            except requests.exceptions.RequestException:
                logger.error(f"Failed to update title for bookmark ID: {bookmark_id}")  # noqa: TRY400


def _perform_bookmark_action(
    client: LinkdingAPIClient,
    bookmark_id: int,
    url: str,
    action_type: Literal["archive", "delete", "unarchive"],
    dry_run: bool,  # noqa: FBT001
    validation_reason: str,
) -> None:
    if dry_run:
        logger.info(
            f"DRY RUN: Would {action_type} bookmark ID: {bookmark_id}, URL: {url} "
            f"because {validation_reason}",
        )
    else:
        try:
            logger.info(
                f"ACTION: {action_type} bookmark ID: {bookmark_id}, URL: {url} "
                f" because {validation_reason}",
            )
            if action_type == "delete":
                client.delete_bookmark(bookmark_id)
            elif action_type == "archive":
                client.archive_bookmark(bookmark_id)
            elif action_type == "unarchive":
                client.unarchive_bookmark(bookmark_id)
        except requests.exceptions.RequestException:
            logger.error(  # noqa: TRY400
                f"Failed to {action_type} bookmark ID: {bookmark_id}, URL: {url}",
            )


class PageChecker:
    def __init__(self) -> None:
        self._check_count = 0
        self._playwright = None
        self._browser: Browser | None = None
        self._initialize_browser()

    def _initialize_browser(self) -> None:
        if self._browser:
            self._browser.close()
        if self._playwright:
            self._playwright.stop()

        self._playwright = sync_playwright().start()
        self._browser = self._playwright.chromium.launch(headless=True)
        logger.debug("Playwright browser initialized.")

    @disk_cache()
    def check_page_load(self, url: str, timeout: int = 30000) -> ValidationResult:
        self._check_count += 1
        if self._check_count >= 100:  # noqa: PLR2004
            logger.debug("Re-initializing Playwright browser after 100 checks.")
            self._initialize_browser()
            self._check_count = 0

        context = self._browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/115.0.0.0 Safari/537.36",
            extra_http_headers={
                "Accept-Language": "en-US,en;q=0.9",
                "Referer": "https://google.com",
            },
        )
        page = context.new_page()
        try:
            response = page.goto(url, wait_until="load", timeout=timeout)

            if response is None:
                return ValidationResult(
                    is_valid=False, reason="No response received (network error?)"
                )

            status = response.status
            if HTTP_OK_MIN <= status < HTTP_OK_MAX:
                return ValidationResult(is_valid=True, reason="OK")
            else:
                return ValidationResult(
                    is_valid=False, reason=f"HTTP status code {status}."
                )

        except Exception as e:
            if "Download is starting" in str(e):
                return ValidationResult(is_valid=True, reason=f"Download: {e!s}")
            else:
                return ValidationResult(is_valid=False, reason=f"Exception: {e!s}")
        finally:
            page.close()

    def close(self) -> None:
        if self._browser:
            self._browser.close()
        if self._playwright:
            self._playwright.stop()


EVALUATE_REDIRECT_REGEXES = re.compile(r"^https://getpocket\.com:443/explore/item/")


def _resolve_redirected_url(
    client: LinkdingAPIClient,
    bookmark: dict,
    dry_run: bool,  # noqa: FBT001
) -> None:
    bookmark_id: int = bookmark["id"]
    original_url: str = bookmark["url"]

    if EVALUATE_REDIRECT_REGEXES.match(original_url):
        logger.debug(f"URL matches redirect pattern: {original_url}")
        try:
            r = requests.get(original_url, timeout=20, allow_redirects=True)
            r.raise_for_status()

            final_url = r.url

            if final_url != original_url:
                if dry_run:
                    logger.info(
                        f"DRY RUN: Would update URL for bookmark ID: {bookmark_id} "
                        f"from '{original_url}' to '{final_url}'",
                    )
                else:
                    try:
                        logger.info(
                            f"Updating URL for bookmark ID: {bookmark_id} "
                            f"from '{original_url}' to '{final_url}'",
                        )
                        client.update_bookmark(bookmark_id, url=final_url)
                    except requests.exceptions.RequestException:
                        logger.error(  # noqa: TRY400
                            f"Failed to update URL for bookmark ID: {bookmark_id}"
                        )
            else:
                logger.debug(f"URL {original_url} did not redirect.")

        except requests.exceptions.RequestException:
            logger.error(  # noqa: TRY400
                f"Failed to resolve redirects for URL: {original_url} "
                f"(ID: {bookmark_id})"
            )


def _process_archived_bookmark(
    client: LinkdingAPIClient,
    pagechecker: PageChecker,
    bookmark: dict,
    dry_run: bool,  # noqa: FBT001
) -> None:
    bookmark_id: int = bookmark["id"]
    url: str = bookmark["url"]
    title: str = bookmark.get("title", "")

    logger.debug(
        f"Processing archived bookmark ID: {bookmark_id}, URL: {url}, title: {title}"
    )

    validation_result = pagechecker.check_page_load(url)
    if validation_result.is_valid:
        _perform_bookmark_action(
            client, bookmark_id, url, "unarchive", dry_run, validation_result.reason
        )
    else:
        logger.debug(
            f"Archived bookmark ID: {bookmark_id} (URL: {url}) is still invalid. "
            f"Reason: {validation_result.reason}"
        )


def _process_unarchived_bookmark(
    client: LinkdingAPIClient,
    pagechecker: PageChecker,
    bookmark: dict,
    dry_run: bool,  # noqa: FBT001
    delete_bookmark: bool,  # noqa: FBT001
) -> None:
    bookmark_id: int = bookmark["id"]
    url: str = bookmark["url"]
    title: str = bookmark.get("title", "")

    logger.debug(f"Processing bookmark ID: {bookmark_id}, URL: {url}, title: {title}")

    action_verb = "delete" if delete_bookmark else "archive"

    validation_result = pagechecker.check_page_load(url)
    if validation_result.is_valid:
        _clean_bookmark_title(client, bookmark, dry_run)
        _resolve_redirected_url(client, bookmark, dry_run)
    else:
        _perform_bookmark_action(
            client, bookmark_id, url, action_verb, dry_run, validation_result.reason
        )


def main() -> None:
    parser = argparse.ArgumentParser(
        description="Clean Linkding bookmarks by archiving or deleting invalid ones.",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Do not archive or delete bookmarks, just show what would be done.",
    )
    parser.add_argument(
        "--delete",
        action="store_true",
        help="Delete bookmarks instead of archiving them.",
    )
    parser.add_argument(
        "--base-url",
        required=True,
        help="Linkding API base URL (e.g., https://linkding.example.com/api/).",
    )
    parser.add_argument(
        "--token",
        required=True,
        help="Linkding API token. Defaults to LINKDING_API_TOKEN environment variable.",
    )
    parser.add_argument(
        "--verbose",
        "-v",
        action="store_true",
        help="Enable verbose output (DEBUG level).",
    )
    args = parser.parse_args()

    if args.verbose:
        logger.setLevel(logging.DEBUG)
        for handler in logger.handlers:
            handler.setLevel(logging.DEBUG)

    client = LinkdingAPIClient(args.base_url, args.token)
    checker = PageChecker()

    logger.info("Starting Linkding bookmark cleaning process...")
    try:
        unarchived_bookmarks = client.get_bookmarks(archived=False)
        for bookmark in tqdm(
            unarchived_bookmarks,
            desc="Processing unarchived bookmarks",
            unit="bookmark",
            smoothing=0,
        ):
            _process_unarchived_bookmark(
                client, checker, bookmark, args.dry_run, args.delete
            )

        archived_bookmarks = client.get_bookmarks(archived=True)
        for bookmark in tqdm(
            archived_bookmarks,
            desc="Processing archived bookmarks",
            unit="bookmark",
            smoothing=0,
        ):
            _process_archived_bookmark(client, checker, bookmark, args.dry_run)

    except requests.exceptions.RequestException as e:
        logger.critical(f"Failed to interact with Linkding API: {e}")
        sys.exit(1)

    checker.close()
    logger.info("Linkding bookmark cleaning process finished.")


if __name__ == "__main__":
    main()
